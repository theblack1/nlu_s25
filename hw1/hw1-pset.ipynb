{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593e287c",
   "metadata": {},
   "source": [
    "# HW 1: Intrinsic Evaluation of Word Embeddings\n",
    "**Due: February 27, 5:00 PM**\n",
    "\n",
    "In this homework assignment, you will implement the 3CosAdd evaluation method<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) for word embeddings described in Section 4 and Subsection 4.1 of [Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781). Please read those sections of the paper now if you have not seen them before.\n",
    "\n",
    "The goal of 3CosAdd is to determine whether word embedding spaces exhibit algebraic relationships between analogous pairs of words. For instance, if we assume that the word _man_ has the same semantic relationship to the word _king_ as _woman_ does to _queen_ (in analogy notation, man : king :: woman : queen), then we might expect that\n",
    "$$\\overrightarrow{\\text{king}} - \\overrightarrow{\\text{man}} \\approx \\overrightarrow{\\text{queen}} - \\overrightarrow{\\text{woman}}$$\n",
    "where $\\overrightarrow{w}$ is the word embedding for word $w$. \n",
    "\n",
    "To measure the extent to which such relations exist, we use a test set that contains a number of analogies $w_1 : w_2 \\mathrel{::} w_3 : w_4$. For each analogy, we ask the following _analogy question_: given $w_1$, $w_2$, and $w_3$, if $w_1 : w_2 \\mathrel{::} w_3 : x$, then what is $x$? We consider the analogy question to have been _answered correctly_ if $\\overrightarrow{w_4}$ happens to be the word embedding vector with the greatest cosine similarity to $\\overrightarrow{w_2} - \\overrightarrow{w_1} + \\overrightarrow{w_3}$. The quality of the embedding space is then measured by its _analogy question accuracy_, given by the proportion of analogy questions that have been answered correctly.\n",
    "\n",
    "## Important: Read Before Starting\n",
    "\n",
    "In the following exercises, you will need to implement functions defined in the `embeddings` and `test_analogies` modules. **Please write all your code in the respective** `.py` **files for those two modules.** You should not submit this notebook with your solutions, and we will not grade it if you do. Please be aware that code written in a Jupyter notebook may run differently when copied into Python modules.\n",
    "\n",
    "The outputs shown in this notebook are the outputs that you should get **when all problems have been completed correctly**. You may obtain different results if you attempt to run the code cells before you have completed the problem set, or if you have completed one or more problems incorrectly. **Obtaining the outputs shown in the code does not guarantee that your code is correct.**\n",
    "\n",
    "To begin, please run the following `import` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50693965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from embeddings import Embeddings\n",
    "from test_analogies import cosine_sim, get_closest_words, load_analogies, \\\n",
    "    run_analogy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e089c",
   "metadata": {},
   "source": [
    "## Problem 1: Load Embeddings (20 Points + 5 EC in Total)\n",
    "\n",
    "In this problem, you will implement the `embeddings.Embeddings` class, which is a container that holds a set of word embeddings. \n",
    "\n",
    "### Problem 1a: Inspect Class Definition and Data Files (No Submission, 0 Points)\n",
    "\n",
    "Please look at the existing code in `embeddings.py`. Take note of the data attributes and methods that have been defined. What is the purpose of the following attributes?\n",
    "* `words`\n",
    "* `indices`\n",
    "* `vectors` \n",
    "* `__len__`\n",
    "* `__contains__`\n",
    "\n",
    "Please also look at the three files `glove_50d.txt`, `glove_100d.txt`, and `glove_200d.txt` in the `data` folder. These files contain GloVe embeddings [(Pennington et al., 2014)](https://aclanthology.org/D14-1162/) of 50, 100, and 200 dimensions, trained on Wikipedia articles and the [Gigaword corpus](https://catalog.ldc.upenn.edu/LDC2011T07).<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2) What is the format of these files?\n",
    "\n",
    "### Problem 1b: Implement the Embeddings Class (Code, 20 Points)\n",
    "\n",
    "The `Embeddings` class contains two methods that are not implemented: the class method `from_file` and the magic method `__getitem__`. Please implement those two methods, making sure to follow the specifications given in their docstrings. When both methods have been implemented, the `Embeddings` class needs to support the following functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0775553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from a file\n",
    "embeddings = Embeddings.from_file(\"data/glove_50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b3c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the number of vectors in the embeddings object\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299c051",
   "metadata": {},
   "source": [
    "5485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834eee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check whether or not a given word has a word embedding\n",
    "print(\"the\" in embeddings)\n",
    "print(\"ds-ga1012\" in embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31e5c0",
   "metadata": {},
   "source": [
    "True  \n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a42c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecs is an ndarray of shape (2, 50).\n",
      "Embedding for 'the': [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "Embedding for 'of': [ 0.70853    0.57088   -0.4716     0.18048    0.54449    0.72603\n",
      "  0.18157   -0.52393    0.10381   -0.17566    0.078852  -0.36216\n",
      " -0.11829   -0.83336    0.11917   -0.16605    0.061555  -0.012719\n",
      " -0.56623    0.013616   0.22851   -0.14396   -0.067549  -0.38157\n",
      " -0.23698   -1.7037    -0.86692   -0.26704   -0.2589     0.1767\n",
      "  3.8676    -0.1613    -0.13273   -0.68881    0.18444    0.0052464\n",
      " -0.33874   -0.078956   0.24185    0.36576   -0.34727    0.28483\n",
      "  0.075693  -0.062178  -0.38988    0.22902   -0.21617   -0.22562\n",
      " -0.093918  -0.80375  ]\n"
     ]
    }
   ],
   "source": [
    "# Index embeddings by a list\n",
    "vecs = embeddings[[\"the\", \"of\"]]\n",
    "print(\"vecs is an {} of shape {}.\".format(type(vecs).__name__, vecs.shape))\n",
    "print(\"Embedding for 'the': {}\".format(vecs[0]))\n",
    "print(\"Embedding for 'of': {}\".format(vecs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ef2b6",
   "metadata": {},
   "source": [
    "vecs is an ndarray of shape (2, 50).  \n",
    "Embedding for 'the': [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02  \n",
    " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01  \n",
    " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01  \n",
    " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01  \n",
    " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01  \n",
    "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03  \n",
    "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01  \n",
    " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01  \n",
    " -1.1514e-01 -7.8581e-01]  \n",
    "Embedding for 'of': [ 0.70853    0.57088   -0.4716     0.18048    0.54449    0.72603  \n",
    "  0.18157   -0.52393    0.10381   -0.17566    0.078852  -0.36216  \n",
    " -0.11829   -0.83336    0.11917   -0.16605    0.061555  -0.012719  \n",
    " -0.56623    0.013616   0.22851   -0.14396   -0.067549  -0.38157  \n",
    " -0.23698   -1.7037    -0.86692   -0.26704   -0.2589     0.1767  \n",
    "  3.8676    -0.1613    -0.13273   -0.68881    0.18444    0.0052464  \n",
    " -0.33874   -0.078956   0.24185    0.36576   -0.34727    0.28483  \n",
    "  0.075693  -0.062178  -0.38988    0.22902   -0.21617   -0.22562  \n",
    " -0.093918  -0.80375  ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf012f4",
   "metadata": {},
   "source": [
    "**Hints:** \n",
    "* `__getitem__` can be implemented using at most 1 line of code.\n",
    "* `from_file` can be implemented using at most 4 lines of code.\n",
    "* Some of the above functionalities are already implemented in the starter code.\n",
    "* Consider using the [`np.fromstring` function in NumPy](https://numpy.org/doc/stable/reference/generated/numpy.fromstring.html).\n",
    "* Please consult one or more of the following resources if you are confused by the Python terminology in this problem: \n",
    " * [Official Python Tutorial on Classes](https://docs.python.org/3/tutorial/classes.html)\n",
    " * [RealPython Tutorial on Object-Oriented Programming](https://realpython.com/python3-object-oriented-programming)\n",
    " * [RealPython Tutorial on Instance, Class, and Static Methods](https://realpython.com/instance-class-and-static-methods-demystified/)\n",
    "\n",
    "\n",
    "### Problem 1c: Extra Credit (Written, 5 Points)\n",
    "\n",
    "Look at the following two lines of code. The first line of code prints the word embeddings $\\overrightarrow{\\text{the}}$ and $\\overrightarrow{\\text{of}}$. The second line of code looks similar to the first line of code, but it does not work as intended. Why is this? \n",
    "\n",
    "**Note:** Depending on how you've implemented the two blank methods in `Embeddings`, it's possible that your code may not behave as shown below. If that's the case, what do you think is the difference between your code and our code that would result in this discrepancy? \n",
    "\n",
    "(Regardless of what happens with the code below, you will receive full credit for Problem 1b as long as all the functional requirements shown above are implemented correctly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54cc3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      "  -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      "  -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      "  -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      "  -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "   4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "   1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      "  -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      "  -1.1514e-01 -7.8581e-01]\n",
      " [ 7.0853e-01  5.7088e-01 -4.7160e-01  1.8048e-01  5.4449e-01  7.2603e-01\n",
      "   1.8157e-01 -5.2393e-01  1.0381e-01 -1.7566e-01  7.8852e-02 -3.6216e-01\n",
      "  -1.1829e-01 -8.3336e-01  1.1917e-01 -1.6605e-01  6.1555e-02 -1.2719e-02\n",
      "  -5.6623e-01  1.3616e-02  2.2851e-01 -1.4396e-01 -6.7549e-02 -3.8157e-01\n",
      "  -2.3698e-01 -1.7037e+00 -8.6692e-01 -2.6704e-01 -2.5890e-01  1.7670e-01\n",
      "   3.8676e+00 -1.6130e-01 -1.3273e-01 -6.8881e-01  1.8444e-01  5.2464e-03\n",
      "  -3.3874e-01 -7.8956e-02  2.4185e-01  3.6576e-01 -3.4727e-01  2.8483e-01\n",
      "   7.5693e-02 -6.2178e-02 -3.8988e-01  2.2902e-01 -2.1617e-01 -2.2562e-01\n",
      "  -9.3918e-02 -8.0375e-01]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This line of code doesn't work:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\2025上-研一下\\1016NLU\\NLUHomework\\nlu_s25\\hw1\\embeddings.py:48\u001b[0m, in \u001b[0;36mEmbeddings.__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03mRetrieves embeddings for a list of words.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    for each i, the ith row is the embedding for words[i]\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# raise NotImplementedError(\"Problem 1b has not been completed yet!\")\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Hint:Write within 1 line\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'h'"
     ]
    }
   ],
   "source": [
    "# This line of code works:\n",
    "print(embeddings[\"the\", \"of\"])\n",
    "\n",
    "# This line of code doesn't work:\n",
    "print(embeddings[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      "  -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      "  -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      "  -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      "  -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "   4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "   1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      "  -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      "  -1.1514e-01 -7.8581e-01]\n",
      " [ 7.0853e-01  5.7088e-01 -4.7160e-01  1.8048e-01  5.4449e-01  7.2603e-01\n",
      "   1.8157e-01 -5.2393e-01  1.0381e-01 -1.7566e-01  7.8852e-02 -3.6216e-01\n",
      "  -1.1829e-01 -8.3336e-01  1.1917e-01 -1.6605e-01  6.1555e-02 -1.2719e-02\n",
      "  -5.6623e-01  1.3616e-02  2.2851e-01 -1.4396e-01 -6.7549e-02 -3.8157e-01\n",
      "  -2.3698e-01 -1.7037e+00 -8.6692e-01 -2.6704e-01 -2.5890e-01  1.7670e-01\n",
      "   3.8676e+00 -1.6130e-01 -1.3273e-01 -6.8881e-01  1.8444e-01  5.2464e-03\n",
      "  -3.3874e-01 -7.8956e-02  2.4185e-01  3.6576e-01 -3.4727e-01  2.8483e-01\n",
      "   7.5693e-02 -6.2178e-02 -3.8988e-01  2.2902e-01 -2.1617e-01 -2.2562e-01\n",
      "  -9.3918e-02 -8.0375e-01]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mfilename_redacted.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This line of code doesn't work:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"the\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32membeddings.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mith\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32m# CODE REDACTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32membeddings.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mith\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32m# CODE REDACTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'h'"
     ]
    }
   ],
   "source": [
    "# This line of code works:\n",
    "print(embeddings[\"the\", \"of\"])\n",
    "\n",
    "# This line of code doesn't work:\n",
    "print(embeddings[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96789c",
   "metadata": {},
   "source": [
    "## Problem 2: Load Testing Data (20 Points in Total)\n",
    "\n",
    "In this problem, you will load and deserialize the analogies dataset used to run the 3CosAdd test.\n",
    "\n",
    "### Problem 2a: Inspect Data File (No Submission, 0 Points)\n",
    "\n",
    "Please inspect the file `data/analogies.txt`, which contains all the analogies used in the 3CosAdd test. What is the format of this file? Notice that the analogies are organized into blocks. Each block contains analogies belonging to a different _relationship type_, and the first row of each block contains the name of the relationship type for that block.\n",
    "\n",
    "### Problem 2b: Implement Data Loading Script (Code, 20 Points)\n",
    "\n",
    "Next, you will implement the function `load_analogies` in the `test_analogies` module. Your code needs to parse the `data/analogies.txt` file and return the data in the form of a `dict`. \n",
    "\n",
    "To understand the format of this dict, please look at lines 43–47 of `test_analogies.py`.<a name=\"cite_ref-3\"></a>[<sup>[3]</sup>](#cite_note-3) This part of the code defines the [type alias](https://docs.python.org/3/library/typing.html#type-aliases) `AnalogiesDataset`, which describes the format in which your code should organize the data. As shown in the following snippet, the keys of an `AnalogiesDataset` should be the names of all the relation types in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeb576b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['capital-common-countries', 'capital-world', 'currency', 'city-in-state', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram4-superlative', 'gram5-present-participle', 'gram6-nationality-adjective', 'gram7-past-tense', 'gram8-plural', 'gram9-plural-verbs'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_analogies(\"data/analogies.txt\")\n",
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['capital-common-countries', 'capital-world', 'currency', 'city-in-state', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram4-superlative', 'gram5-present-participle', 'gram6-nationality-adjective', 'gram7-past-tense', 'gram8-plural', 'gram9-plural-verbs'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_analogies(\"data/analogies.txt\")\n",
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef7c85",
   "metadata": {},
   "source": [
    "For each relation type, the corresponding value in the `AnalogiesDataset` should be a list containing all the analogies under that relation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abuja', 'nigeria', 'accra', 'ghana')\n",
      "('abuja', 'nigeria', 'algiers', 'algeria')\n",
      "('abuja', 'nigeria', 'amman', 'jordan')\n",
      "('abuja', 'nigeria', 'ankara', 'turkey')\n",
      "('abuja', 'nigeria', 'antananarivo', 'madagascar')\n",
      "('abuja', 'nigeria', 'apia', 'samoa')\n",
      "('abuja', 'nigeria', 'ashgabat', 'turkmenistan')\n",
      "('abuja', 'nigeria', 'asmara', 'eritrea')\n",
      "('abuja', 'nigeria', 'astana', 'kazakhstan')\n",
      "('abuja', 'nigeria', 'athens', 'greece')\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 analogies for the relation type capital-world\n",
    "for analogy in test_data[\"capital-world\"][:10]:\n",
    "    print(analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e96e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abuja', 'nigeria', 'accra', 'ghana')\n",
      "('abuja', 'nigeria', 'algiers', 'algeria')\n",
      "('abuja', 'nigeria', 'amman', 'jordan')\n",
      "('abuja', 'nigeria', 'ankara', 'turkey')\n",
      "('abuja', 'nigeria', 'antananarivo', 'madagascar')\n",
      "('abuja', 'nigeria', 'apia', 'samoa')\n",
      "('abuja', 'nigeria', 'ashgabat', 'turkmenistan')\n",
      "('abuja', 'nigeria', 'asmara', 'eritrea')\n",
      "('abuja', 'nigeria', 'astana', 'kazakhstan')\n",
      "('abuja', 'nigeria', 'athens', 'greece')\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 analogies for the relation type capital-world\n",
    "for analogy in test_data[\"capital-world\"][:10]:\n",
    "    print(analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e575385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries: ('athens', 'greece', 'baghdad', 'iraq'), 505 more analogies...\n",
      "capital-world: ('abuja', 'nigeria', 'accra', 'ghana'), 4523 more analogies...\n",
      "currency: ('algeria', 'dinar', 'angola', 'kwanza'), 865 more analogies...\n",
      "city-in-state: ('chicago', 'illinois', 'houston', 'texas'), 2466 more analogies...\n",
      "family: ('boy', 'girl', 'brother', 'sister'), 505 more analogies...\n",
      "gram1-adjective-to-adverb: ('amazing', 'amazingly', 'apparent', 'apparently'), 991 more analogies...\n",
      "gram2-opposite: ('acceptable', 'unacceptable', 'aware', 'unaware'), 811 more analogies...\n",
      "gram3-comparative: ('bad', 'worse', 'big', 'bigger'), 1331 more analogies...\n",
      "gram4-superlative: ('bad', 'worst', 'big', 'biggest'), 1121 more analogies...\n",
      "gram5-present-participle: ('code', 'coding', 'dance', 'dancing'), 1055 more analogies...\n",
      "gram6-nationality-adjective: ('albania', 'albanian', 'argentina', 'argentinean'), 1598 more analogies...\n",
      "gram7-past-tense: ('dancing', 'danced', 'decreasing', 'decreased'), 1559 more analogies...\n",
      "gram8-plural: ('banana', 'bananas', 'bird', 'birds'), 1331 more analogies...\n",
      "gram9-plural-verbs: ('decrease', 'decreases', 'describe', 'describes'), 869 more analogies...\n"
     ]
    }
   ],
   "source": [
    "# Peek at the analogies under each relation type\n",
    "for k, v in test_data.items():\n",
    "    print(\"{}: {}, {} more analogies...\".format(k, v[0], len(v) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba52716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries: ('athens', 'greece', 'baghdad', 'iraq'), 505 more analogies...\n",
      "capital-world: ('abuja', 'nigeria', 'accra', 'ghana'), 4523 more analogies...\n",
      "currency: ('algeria', 'dinar', 'angola', 'kwanza'), 865 more analogies...\n",
      "city-in-state: ('chicago', 'illinois', 'houston', 'texas'), 2466 more analogies...\n",
      "family: ('boy', 'girl', 'brother', 'sister'), 505 more analogies...\n",
      "gram1-adjective-to-adverb: ('amazing', 'amazingly', 'apparent', 'apparently'), 991 more analogies...\n",
      "gram2-opposite: ('acceptable', 'unacceptable', 'aware', 'unaware'), 811 more analogies...\n",
      "gram3-comparative: ('bad', 'worse', 'big', 'bigger'), 1331 more analogies...\n",
      "gram4-superlative: ('bad', 'worst', 'big', 'biggest'), 1121 more analogies...\n",
      "gram5-present-participle: ('code', 'coding', 'dance', 'dancing'), 1055 more analogies...\n",
      "gram6-nationality-adjective: ('albania', 'albanian', 'argentina', 'argentinean'), 1598 more analogies...\n",
      "gram7-past-tense: ('dancing', 'danced', 'decreasing', 'decreased'), 1559 more analogies...\n",
      "gram8-plural: ('banana', 'bananas', 'bird', 'birds'), 1331 more analogies...\n",
      "gram9-plural-verbs: ('decrease', 'decreases', 'describe', 'describes'), 869 more analogies...\n"
     ]
    }
   ],
   "source": [
    "# Peek at the analogies under each relation type\n",
    "for k, v in test_data.items():\n",
    "    print(\"{}: {}, {} more analogies...\".format(k, v[0], len(v) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d228c",
   "metadata": {},
   "source": [
    "To receive full credit, the output of your function must satisfy the following requirements.\n",
    "* The keys of the `dict` must exactly match the names of the relation types listed in the `data/analogies.txt` data file. The keys should not include the initial `: ` in the names of the relation types, and they should not contain any leading or trailing whitespace.\n",
    "* Each analogy must be represented as a `tuple` or `list` of exactly 4 strings.\n",
    "\n",
    "## Problem 3: Implement 3CosAdd (40 Points in Total)\n",
    "\n",
    "For this problem, you will implement code that answers analogy questions and computes the analogy question accuracy attained by a set of word embeddings on each relation type.\n",
    "\n",
    "### Problem 3a: Inspect test_analogies Module (No Submission, 0 Points)\n",
    "\n",
    "You will now implement all the functions defined in the `test_analogies` module apart from `load_analogies`, which you should have implemented in Problem 2b. There are three functions in total:\n",
    "* `cosine_sim`, which computes cosine similarity\n",
    "* `get_closest_words`, which finds the closest words (in terms of cosine similarity) to a given vector in the embedding space\n",
    "* `run_analogy_test`, which runs 3CosAdd on a set of word embeddings (represented by the `Embeddings` object).\n",
    "\n",
    "### Problem 3b: Calculate Cosine Similarity (Code, 10 Points)\n",
    "\n",
    "Please implement the function `cosine_sim`, referring to the docstring and the following code snippet for guidance. The function should take two matrices of row vectors and compute the cosine similarity between every row of the first matrix and every row of the second matrix. Recall that the cosine similarity between two vectors $\\mathbf{u}$ and $\\mathbf{v}$ is given by\n",
    "$$ \\cos(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top\\mathbf{v}}{\\lVert \\mathbf{u} \\rVert \\cdot \\lVert \\mathbf{v} \\rVert}\\text{;} $$\n",
    "in other words, it is the dot product of $\\mathbf{u}$ and $\\mathbf{v}$ when they are normalized to unit length.\n",
    "\n",
    "To receive full credit, your code must support the following usage. After line 8 is executed, `sims[i, j]` must contain the cosine similarity between `a[i]` and `b[j]` for each `i` and `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Cosine Similarities:\n",
      "[[ 0.01415985  0.64383657  0.33316909]\n",
      " [-0.49852156 -0.27163371 -0.2392439 ]]\n",
      "\n",
      "cos([ 2  5  9 10], [-6  7  1 -3]) = 0.014159846508095777\n",
      "cos([ 2  5  9 10], [-3  7  9  0]) = 0.6438365650063621\n",
      "cos([ 2  5  9 10], [-6 10 10 -5]) = 0.3331690892566789\n",
      "cos([ 0 -8  3 -3], [-6  7  1 -3]) = -0.49852156261828695\n",
      "cos([ 0 -8  3 -3], [-3  7  9  0]) = -0.2716337139226146\n",
      "cos([ 0 -8  3 -3], [-6 10 10 -5]) = -0.23924389509855973\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2, 5, 9, 10], \n",
    "              [0, -8, 3, -3]])\n",
    "b = np.array([[-6, 7, 1, -3],\n",
    "              [-3, 7, 9, 0],\n",
    "              [-6, 10, 10, -5]])\n",
    "\n",
    "# Full output of cosine_sim\n",
    "sims = cosine_sim(a, b)\n",
    "print(\"All Cosine Similarities:\\n{}\\n\".format(sims))\n",
    "\n",
    "# Rows correspond to rows of a, columns correspond to rows of b\n",
    "for i, j in itertools.product(range(len(a)), range(len(b))):\n",
    "    print(\"cos({}, {}) = {}\".format(a[i], b[j], sims[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551b88b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Cosine Similarities:\n",
      "[[ 0.01415985  0.64383657  0.33316909]\n",
      " [-0.49852156 -0.27163371 -0.2392439 ]]\n",
      "\n",
      "cos([ 2  5  9 10], [-6  7  1 -3]) = 0.014159846508095777\n",
      "cos([ 2  5  9 10], [-3  7  9  0]) = 0.6438365650063621\n",
      "cos([ 2  5  9 10], [-6 10 10 -5]) = 0.3331690892566789\n",
      "cos([ 0 -8  3 -3], [-6  7  1 -3]) = -0.49852156261828695\n",
      "cos([ 0 -8  3 -3], [-3  7  9  0]) = -0.2716337139226146\n",
      "cos([ 0 -8  3 -3], [-6 10 10 -5]) = -0.23924389509855973\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2, 5, 9, 10], \n",
    "              [0, -8, 3, -3]])\n",
    "b = np.array([[-6, 7, 1, -3],\n",
    "              [-3, 7, 9, 0],\n",
    "              [-6, 10, 10, -5]])\n",
    "\n",
    "# Full output of cosine_sim\n",
    "sims = cosine_sim(a, b)\n",
    "print(\"All Cosine Similarities:\\n{}\\n\".format(sims))\n",
    "\n",
    "# Rows correspond to rows of a, columns correspond to rows of b\n",
    "for i, j in itertools.product(range(len(a)), range(len(b))):\n",
    "    print(\"cos({}, {}) = {}\".format(a[i], b[j], sims[i, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873993d",
   "metadata": {},
   "source": [
    "**Hint:** `cosine_sim` can be implemented in **at most 9 lines of code**. The performance of your code may suffer if it is substantially longer than this!\n",
    "\n",
    "### Problem 3c: Find Neighboring Words (Code, 10 Points)\n",
    "\n",
    "Next, please implement the function `get_closest_words`. This function finds the `k` closest words to one or more given vector(s) in the embedding space. The given vectors do not have to be valid word embeddings (though they do need to have the same dimension as the word embeddings), and the `k` closest words do not have to be in order.\n",
    "\n",
    "For example, this is how you would find the 4 nearest neighbors of _king_, _man_, and _woman_ in the embedding space. (Note that each word is one of its own 4 nearest neighbors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b097ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ii', 'king', 'queen', 'prince'],\n",
       " ['man', 'woman', 'boy', 'another'],\n",
       " ['woman', 'girl', 'man', 'mother']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:  # Load embeddings if they haven't been loaded yet\n",
    "    embeddings\n",
    "except NameError:\n",
    "    embeddings = Embeddings.from_file(\"data/glove_50d.txt\")\n",
    "    \n",
    "# Find the neighbors of \"king,\" \"man,\" and \"woman\"\n",
    "vecs = embeddings[\"king\", \"man\", \"woman\"]\n",
    "get_closest_words(embeddings, vecs, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e31eda",
   "metadata": {},
   "source": [
    "[['ii', 'king', 'queen', 'prince'],  \n",
    " ['man', 'woman', 'boy', 'another'],  \n",
    " ['woman', 'girl', 'man', 'mother']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47bc036",
   "metadata": {},
   "source": [
    "The output above is interpreted as follows:\n",
    "* the 4 nearest neighbors of _king_ are _ii_, _king_, _queen_, and _prince_\n",
    "* the 4 nearest neighbors of _man_ are _man_, _woman_, _boy_, and _another_\n",
    "* the 4 nearest neighbors of _woman_ are _woman_, _girl_, _man_, and _mother_.\n",
    "\n",
    "Once you have implemented `get_closest_words`, you will use it to answer analogy questions. For instance, the analogy question for man : king :: woman : queen (if $\\text{man} : \\text{king} \\mathrel{::} \\text{woman} : x$, then what is $x$?) can be answered as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['woman']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the neighbors of king - man + woman\n",
    "# king - man = queen - woman\n",
    "# woman = queen - king + man\n",
    "\n",
    "# queen = vecs[0:1] - vecs[1:2] + vecs[2:3]\n",
    "# get_closest_words(embeddings, queen, k=4)\n",
    "\n",
    "woman = vecs[2:3] - vecs[0:1] + vecs[1:2]\n",
    "get_closest_words(embeddings, woman, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2f9ce",
   "metadata": {},
   "source": [
    "[['prince', 'queen', 'daughter', 'king']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52174e9",
   "metadata": {},
   "source": [
    "In this instance, we are being a bit more _lenient_ than Mikolov et al. (2013): we will consider the analogy question to have been answered correctly because $\\overrightarrow{\\text{queen}}$ is one of the **top 4 closest** word embeddings to $\\overrightarrow{\\text{king}} - \\overrightarrow{\\text{man}} + \\overrightarrow{\\text{woman}}$, even though Mikolov et al. required it to be **the closest** word embedding (which in case happens to be $\\overrightarrow{\\text{king}}$). \n",
    "\n",
    "**Hints:**\n",
    "* `get_closest_words` can be implemented in **at most 3 lines of code**.\n",
    "* You can make your code run faster if you don't try to return the closest words in order (though the difference in performance may not be noticeable since we are using a small number of embeddings).\n",
    "\n",
    "### Problem 3d: Write Testing Script (Code, 20 Points)\n",
    "\n",
    "Finally, please implement the function `run_analogy_test`. Your code should run the 3CosAdd test on a given embedding space and testing dataset. The output of your code, illustrated below, should be a dict containing the analogy question accuracy for analogies from each relation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d909d8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital-common-countries': 0.6837944664031621,\n",
       " 'capital-world': 0.5899646330680813,\n",
       " 'currency': 0.08429561200923788,\n",
       " 'city-in-state': 0.08755573571139036,\n",
       " 'family': 0.4762845849802372,\n",
       " 'gram1-adjective-to-adverb': 0.09173387096774194,\n",
       " 'gram2-opposite': 0.08866995073891626,\n",
       " 'gram3-comparative': 0.26876876876876876,\n",
       " 'gram4-superlative': 0.2531194295900178,\n",
       " 'gram5-present-participle': 0.0928030303030303,\n",
       " 'gram6-nationality-adjective': 0.8667917448405253,\n",
       " 'gram7-past-tense': 0.09358974358974359,\n",
       " 'gram8-plural': 0.25900900900900903,\n",
       " 'gram9-plural-verbs': 0.1896551724137931}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from embeddings import Embeddings\n",
    "from test_analogies import cosine_sim, get_closest_words, load_analogies, \\\n",
    "    run_analogy_test\n",
    "test_data = load_analogies(\"data/analogies.txt\")\n",
    "embeddings = Embeddings.from_file(\"data/glove_50d.txt\")\n",
    "# Run the analogy test\n",
    "run_analogy_test(embeddings, test_data, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37cbf4",
   "metadata": {},
   "source": [
    "{'capital-common-countries': 0.6837944664031621,  \n",
    " 'capital-world': 0.5899646330680813,  \n",
    " 'currency': 0.08429561200923788,  \n",
    " 'city-in-state': 0.08755573571139036,  \n",
    " 'family': 0.4762845849802372,  \n",
    " 'gram1-adjective-to-adverb': 0.09173387096774194,  \n",
    " 'gram2-opposite': 0.08866995073891626,  \n",
    " 'gram3-comparative': 0.26876876876876876,  \n",
    " 'gram4-superlative': 0.2531194295900178,  \n",
    " 'gram5-present-participle': 0.0928030303030303,  \n",
    " 'gram6-nationality-adjective': 0.8667917448405253,  \n",
    " 'gram7-past-tense': 0.09358974358974359,  \n",
    " 'gram8-plural': 0.25900900900900903,  \n",
    " 'gram9-plural-verbs': 0.1896551724137931}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffb63c",
   "metadata": {},
   "source": [
    "**Hint:** `run_analogy_test` can be implemented in **at most 9 lines of code**. The performance of your code may suffer if it is substantially longer than this! \n",
    "\n",
    "## Problem 4: Interpretation of Results (20 Points in Total)\n",
    "\n",
    "In the final part of this assignment, you will use your code to study the three word embedding spaces provided in the assignment's GitHub repository. You may (optionally) wish to write additional code for the following problems, but you do not need to submit it.\n",
    "\n",
    "### Problem 4a: Syntactic vs. Semantic Relation Types (Written, 10 Points)\n",
    "\n",
    "In [Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781), the 14 relation types are grouped into two categories: _semantic_ relation types and _syntactic_ relation types. Please look through the paper and find out which relation types are semantic and which are syntactic.\n",
    "\n",
    "Now, please run the 3CosAdd test with a lenience of `k = 1` on all three embedding spaces (the 50-, 100-, and 200-dimensional embeddings), and then answer the following questions.\n",
    "1. What is the total analogy question accuracy obtained by each embedding space for all analogies belonging belonging to a **semantic** relation type?\n",
    "2. What is the total analogy question accuracy obtained by each embedding space for all analogies belonging belonging to a **syntactic** relation type?\n",
    "3. What is the total analogy question accuracy obtained by each embedding space for **all analogies** in the testing dataset?\n",
    "4. How do your GloVe results compare to the results for the two word2vec models (Skip-Gram and CBOW) reported in Table 4 of Mikolov et al. (2013)? Does the dimensionality of the embedding space have any effect on analogy question accuracy?\n",
    "\n",
    "The only thing you need to submit for this problem is your answers to the above bullet points. For questions 1–3 you are encouraged to report your results in the form of a table such as the following. **Please report all numerical results with 3 significant figures of precision.**\n",
    "\n",
    "| Embedding Space | Semantic | Syntactic | Overall |\n",
    "|-----------------|----------|-----------|---------|\n",
    "| GloVe 50        |          |           |         |\n",
    "| GloVe 100       |          |           |         |\n",
    "| GloVe 200       |          |           |         |\n",
    "\n",
    "### Problem 4b: Effect of Lenience (Written, 5 Points)\n",
    "\n",
    "Please repeat Problem 4a using a lenience of `k = 2`. Do the results change? If so, how?\n",
    "\n",
    "### Problem 4c: Qualitative Evaluation (Written, 5 Points)\n",
    "\n",
    "For the final problem, you will replicate Table 8 of Mikolov et al. (2013) by looking at some examples of answers to analogy questions. For each of the three embedding spaces, please answer the following analogy questions. \n",
    "* france : paris :: italy : $x$\n",
    "* france : paris :: japan : $x$\n",
    "* france : paris :: florida : $x$\n",
    "* big : bigger :: small : $x$\n",
    "* big : bigger :: cold : $x$\n",
    "* big : bigger :: quick : $x$\n",
    "\n",
    "You are encouraged to present your answer as a table like the following.\n",
    "\n",
    "| Analogy Question                | Gold Answer  | GloVe 50 | GloVe 100 | GloVe 200 |\n",
    "|---------------------------------|--------------|----------|-----------|-----------|\n",
    "| france : paris :: italy : _x_   | rome         |          |           |           |\n",
    "| france : paris :: japan : _x_   | tokyo        |          |           |           |\n",
    "| france : paris :: florida : _x_ | tallahassee  |          |           |           |\n",
    "| big : bigger :: small : _x_     | smaller      |          |           |           |\n",
    "| big : bigger :: cold : _x_      | colder       |          |           |           |\n",
    "| big : bigger :: quick : _x_     | quicker      |          |           |           |\n",
    "\n",
    "\n",
    "Please comment on your results. How do the different embedding spaces compare to one another? How do they compare to the results reported by Mikolov et al.?\n",
    "\n",
    "\n",
    "## Footnotes\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [<sup>^</sup>](#cite_ref-1) This terminology is due to [Drodz et al. (2016)](https://aclanthology.org/C16-1332/); Mikolov et al. (2013) call it the \"vector offset method.\"\n",
    "\n",
    "<a name=\"cite_note-2\"></a>2. [<sup>^</sup>](#cite_ref-2) To make the assignment easier to download, the word embedding files have been truncated to a manageable size. The full word embedding files are available [here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "<a name=\"cite_note-3\"></a>3. [<sup>^</sup>](#cite_ref-3) Obviously, these line numbers may be inaccurate if you have already started working on Problem 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
